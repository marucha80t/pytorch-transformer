# Transformer: Attention is all you need
An implementation of transfomer refer to [Attention is all you need (NIPS2017)](https://arxiv.org/abs/1706.03762).

<p align=center>
<img src=https://user-images.githubusercontent.com/53220859/65560102-40ef3980-df78-11e9-8b4f-7ea5c09a6061.png width=350pt>
</p>

## Settings
This code are depend on the following.

- python==3.6.5
- pytorch==1.1.0
- torchtext==0.3.1

```sh
git clone https://github.com/t080/pytorch-transformer.git
cd ./pytorch-transformer
pip install -r requirements.txt
```

## Usages
### Training
The `--train` and `--valid` options receive the path to a data file for training and validation, respectively. The data file must be tab-separated values (TSV) format. If you need to use GPU, please set the `--gpu` option.

```sh
python train.py \
  --train ./data/sample_train.tsv \
  --valid ./data/sample_valid.tsv \
  --savedir ./checkpoints \
  --gpu
```

### Translation
The --model option receives the path to a model file generated by train.py. A text file that you want to translate is given to --input. If you need to use GPU, please set the --gpu option.

```sh
python translate.py \
  --model ./checkpoints/checkpoint_best.pt \
  --input ./data/sample_test.txt \
  --gpu
```

## References
- [Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems 2017, pp. 5998-6008.](https://arxiv.org/abs/1706.03762)
